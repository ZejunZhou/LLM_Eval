# CS 638 Large Language Model Learnning Ability Evaluation

Link to some past notes: https://docs.google.com/document/d/1X4dEgaM8u3R81fOU6_islTFzp03h86bdzLc2c_fmX0U/edit?usp=sharing <br>
Link to dataset: https://huggingface.co/datasets/Amod/mental_health_counseling_conversations

Model plan to evaluate: ChatGPT, Cohort, Llama

Goal: Evaluate the learning ability of Large Langauge Models, specifically in **healthcare advice topics**.

Steps to take:
1.  check how to feed our data to the three models
2.  the **first step** only feed questions, record the answer
3.  manual evaluate the "quality" of response; rank performance, record down pros / cons / features of response given from three model
4.  then **the second step** is to "train" the model using expected response in dataset
5.  evaluation, manual check the quality of response given from all three models, rank performance, record down new pros / cons / features of response given from three model.
6.  Write these into report

## Report Structure
1. Introduction
2. Pre-Evaluation
3. Training (briefly describe process)
4. Post-Evaluation
5. Conclusion & Discussion
